{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ef5ea60-8537-407b-a234-c3f0a4254eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics \n",
    "from rouge import Rouge \n",
    "from tqdm import tqdm\n",
    "\n",
    "from parser import *\n",
    "from utils import *\n",
    "from models import Decoder\n",
    "from datasets import HistDataset\n",
    "from run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32b6d172-646e-413d-a50d-6ca671d9e25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)\n",
      "Namespace(alpha=0.2, batch_size=32, beam_size=2, ctx_attn='sum_co', dataset='example', dropout=0.6, epochs=2, gpu=False, hidden_size=128, integration='', lamb=0.05, lr=0.003, n_ctxs=2, n_features=128, neg=2, nheads=4, optim='Adam', resume=False, task='multi', weight_decay=5e-06)\n",
      "data_load_over!\n"
     ]
    }
   ],
   "source": [
    "opt = get_parser()\n",
    "# loader = Loader(opt.dataset, opt.gpu)\n",
    "loader = Loader(\"example\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b690c6e7-0b59-41f5-b519-5e7fd70bd51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:\n",
      "   paper_id1  paper_id2                                      clean_content  \\\n",
      "0          0         55  obviously they satisfy the constraints fullfco...   \n",
      "1          0         53  2table 2 bleu 4 scores with one reference tran...   \n",
      "2          0         24  note that the re sulting conditional distribut...   \n",
      "3          0         89  the learning mod els are support vector machin...   \n",
      "4          0         56  we note here other ex amples described their a...   \n",
      "\n",
      "   num_of_words  \n",
      "0            97  \n",
      "1           103  \n",
      "2           100  \n",
      "3           128  \n",
      "4            46  \n",
      "test_data:\n",
      "     paper_id1  paper_id2                                      clean_content  \\\n",
      "821         82         19  once the feature values are computed our goal ...   \n",
      "771         77         69  next we extracted phrase level translation pai...   \n",
      "954         95         92  the stanford parser 13 and crf chunker 14 have...   \n",
      "631         63         50  g k best forest oracles orhopfear derivations ...   \n",
      "236         23         46  1 thebaseline is a state of the art phrase bas...   \n",
      "\n",
      "     num_of_words  \n",
      "821            76  \n",
      "771           117  \n",
      "954           107  \n",
      "631            45  \n",
      "236            61  \n"
     ]
    }
   ],
   "source": [
    "print(\"train_data:\")\n",
    "print(loader.train_data.head())\n",
    "\n",
    "# 查看 test_data 的前几行数据\n",
    "print(\"test_data:\")\n",
    "print(loader.test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e57f4c3-0578-43ff-a77a-fcc3bc75d6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "print(loader.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30389e8a-205c-4aff-982e-1edbde69d5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "word = loader.build_word(\"once\")\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac7ecb7a-e367-4d71-95a5-f65a457d34af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.idx2word[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9ac6067-1749-459d-abd9-3faf9052d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_out= loader.G_out\n",
    "# G_out.nodes()：返回图中所有节点的列表。\n",
    "# G_out.edges()：返回图中所有边的列表。\n",
    "# G_out.adj[node]：返回与节点 node 相邻的所有节点的字典，字典的键是相邻节点的标识符，值是边的属性字典。\n",
    "# G_out.degree(node)：返回节点 node 的度数（即与该节点相邻的边的数量）。\n",
    "# G_out.number_of_nodes()：返回图中节点的数量。\n",
    "# G_out.number_of_edges()：返回图中边的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f81c06ff-40f5-40e5-8b86-6e2df1e2d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "962\n"
     ]
    }
   ],
   "source": [
    "print(len(G_out.nodes()))\n",
    "print(len(G_out.edges()))\n",
    "# for u, v, d in G_out.edges(data=True):\n",
    "#     print(u, v, d['context'])\n",
    "# 0 55 4 1 inference based learningmany learning paradigms can be defined asinference based learning these include the per ceptronand its large margin vari ants in these settings a models parameters areiteratively updated based on the argmax calculationfor a single or set of training instances under thecurrent parameter settings\n",
    "# 0 53 2table 2 bleu 4 scores with one reference translating into english the numbers in parentheses are times inhours to run parameter optimization end to end refers to moses linear model features extended refers to non linear and hidden state features polynomial features future cost dl refers to distortion limit search is the set ofparameters controlling search quality parameters controlling beam size histogram pruning and threshold pruningour baseline system is trained for each languagepair by running minimum error rate trainingon 1000 sentences each iteration of mertutilizes 19 random starting points plus the points ofconvergence at all previous iterations of mert anda uniform weight vector\n",
    "# 0 24 note that the re sulting conditional distribution will be drawn solelyfrom one input distribution when the conditioningcontext is unseen in the remaining distributions thismay lead to an over reliance on unreliable distribu tions which can be ameliorated by smoothing as an alternative to linear interpolation we alsoemploy a weighted product for phrase table combi nation p jpjj 3this has the same form used for log linear trainingof smt decoderswhich allows us totreat each distribution as a feature and learn the mix ing weights automatically note that we must indi vidually smooth the component distributions in to stop zeros from propagating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229be9d6-c448-4bd6-a3c4-5593555e6af2",
   "metadata": {},
   "source": [
    "# 构建模型对象Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e746fa9a-afca-4edc-b8c9-67bbd54ea96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decoder(n_words=loader.n_words,  # 9860\n",
    "                    n_nodes=loader.n_nodes,  # 101\n",
    "                    max_len=loader.max_len,  # 191\n",
    "                    opt = opt)\n",
    "if opt.gpu: model= model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfd9d2e3-6af6-44c0-83cc-55645df341a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0332,  0.1050,  1.0852,  ..., -0.2843,  1.7697, -0.0296],\n",
      "        [-0.0047, -0.7061, -0.9779,  ..., -1.2059, -1.5961,  1.4125],\n",
      "        [ 1.5672, -0.3576, -0.1071,  ..., -0.1798,  1.1636, -1.1166],\n",
      "        ...,\n",
      "        [-0.8575,  1.1274,  1.0805,  ..., -0.9891,  0.0766,  2.3136],\n",
      "        [-0.3479,  1.2266, -0.3459,  ...,  2.9780,  0.9717, -0.5160],\n",
      "        [ 1.8138,  1.6081,  0.5013,  ..., -0.2186, -0.2150, -0.0863]]))])\n"
     ]
    }
   ],
   "source": [
    "# 模型的参数通常保存在一个叫做“状态字典”（state dictionary）的对象中。state_dict() 方法可以返回模型的状态字典，其中包含了所有模型的参数及其对应的值\n",
    "print(model.node_embedding.state_dict()) # weight 是 node_embedding 的参数名，tensor 对象则是参数的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ff9bb03-4017-455c-b229-298411d9ebd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([101, 128])\n",
      "torch.Size([9860, 128])\n"
     ]
    }
   ],
   "source": [
    "node_embedding_state_dict = model.node_embedding.state_dict()\n",
    "node_weight_shape = node_embedding_state_dict['weight'].shape\n",
    "print(node_weight_shape)\n",
    "word_embedding_state_dict = model.word_embedding.state_dict()\n",
    "word_weight_shape = word_embedding_state_dict['weight'].shape\n",
    "print(word_weight_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae7393e7-e931-49fa-9720-ba903de0867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.optim == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=opt.lr)\n",
    "# Adagrad 优化器适合处理具有稀疏梯度和欠约束的目标函数\n",
    "elif opt.optim == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=opt.lr)\n",
    "# SGD 优化器则更适合处理大规模数据集和稳定的目标函数\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=opt.lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88fab19-24f3-48f4-809b-cf7f14f151f3",
   "metadata": {},
   "source": [
    "# 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c07ddf5-51b6-4e13-9229-7fae05539b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "train_dataset = HistDataset(loader, opt) # 数据集相关 负样本 划分等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a2a9aff-ab70-449c-aa69-a56198a615d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.n_ctxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3962e0-f153-4ac3-9cab-583469c5c208",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4eb6694e-6fd4-4660-aea6-a50440cbda8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset,\n",
    "                          opt.batch_size,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=loader.collate_fun,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38204308-a862-4a54-89aa-116c29b07372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1]:   0%|                                         | 0/32 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "Caught NameError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/root/autodl-tmp/AutoCite-main/utils.py\", line 158, in collate_fun\n    input_lengths = input_lengths.cuda()\nNameError: name 'h_idx' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_97/524113676.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/autodl-tmp/AutoCite-main/run.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, epochs, optimizer)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0m_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'{desc}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[Epoch {}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_of_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: Caught NameError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/root/autodl-tmp/AutoCite-main/utils.py\", line 158, in collate_fun\n    input_lengths = input_lengths.cuda()\nNameError: name 'h_idx' is not defined\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dl, opt.epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7108680d-44d9-40bd-8082-4a1b83a2ef78",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.core.indexes.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_107/3937894333.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"example\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas.core.indexes.numeric'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "path = 'data/{}.pkl'.format(\"example\")\n",
    "train_data, test_data = pickle.load(open(path, 'rb'))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a262cdab-edf9-432a-8b2b-5c6fa9a7558a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autocite",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
